{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b24debc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUDHAKAR\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "C:\\Users\\SUDHAKAR\\AppData\\Local\\Temp\\ipykernel_20328\\3218802279.py:54: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 708s 2s/step - loss: 1.7985 - accuracy: 0.2637 - val_loss: 1.6883 - val_accuracy: 0.3334\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 295s 659ms/step - loss: 1.6203 - accuracy: 0.3710 - val_loss: 1.5362 - val_accuracy: 0.4129\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 509s 1s/step - loss: 1.5311 - accuracy: 0.4119 - val_loss: 1.4635 - val_accuracy: 0.4448\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 569s 1s/step - loss: 1.4606 - accuracy: 0.4408 - val_loss: 1.4084 - val_accuracy: 0.4672\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 562s 1s/step - loss: 1.4033 - accuracy: 0.4618 - val_loss: 1.3592 - val_accuracy: 0.4817\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 713s 2s/step - loss: 1.3519 - accuracy: 0.4849 - val_loss: 1.3190 - val_accuracy: 0.5020\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 220s 490ms/step - loss: 1.3109 - accuracy: 0.5033 - val_loss: 1.2822 - val_accuracy: 0.5155\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 214s 477ms/step - loss: 1.2653 - accuracy: 0.5184 - val_loss: 1.2506 - val_accuracy: 0.5283\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 213s 476ms/step - loss: 1.2320 - accuracy: 0.5341 - val_loss: 1.2198 - val_accuracy: 0.5356\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 220s 490ms/step - loss: 1.2005 - accuracy: 0.5466 - val_loss: 1.2021 - val_accuracy: 0.5375\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 233s 520ms/step - loss: 1.1681 - accuracy: 0.5586 - val_loss: 1.1844 - val_accuracy: 0.5506\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 258s 575ms/step - loss: 1.1460 - accuracy: 0.5690 - val_loss: 1.1640 - val_accuracy: 0.5579\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 239s 533ms/step - loss: 1.1209 - accuracy: 0.5813 - val_loss: 1.1529 - val_accuracy: 0.5600\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 257s 574ms/step - loss: 1.0931 - accuracy: 0.5912 - val_loss: 1.1415 - val_accuracy: 0.5695\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 261s 584ms/step - loss: 1.0681 - accuracy: 0.6014 - val_loss: 1.1262 - val_accuracy: 0.5762\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 266s 593ms/step - loss: 1.0479 - accuracy: 0.6109 - val_loss: 1.1155 - val_accuracy: 0.5781\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 276s 617ms/step - loss: 1.0212 - accuracy: 0.6194 - val_loss: 1.1091 - val_accuracy: 0.5776\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 251s 560ms/step - loss: 1.0009 - accuracy: 0.6285 - val_loss: 1.1001 - val_accuracy: 0.5841\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 219s 488ms/step - loss: 0.9794 - accuracy: 0.6373 - val_loss: 1.0964 - val_accuracy: 0.5851\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 218s 488ms/step - loss: 0.9577 - accuracy: 0.6487 - val_loss: 1.0804 - val_accuracy: 0.5949\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 221s 494ms/step - loss: 0.9339 - accuracy: 0.6556 - val_loss: 1.0819 - val_accuracy: 0.5922\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 218s 487ms/step - loss: 0.9093 - accuracy: 0.6643 - val_loss: 1.0886 - val_accuracy: 0.5904\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 310s 692ms/step - loss: 0.8877 - accuracy: 0.6737 - val_loss: 1.0789 - val_accuracy: 0.5977\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 2694s 6s/step - loss: 0.8640 - accuracy: 0.6833 - val_loss: 1.0783 - val_accuracy: 0.5989\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 218s 486ms/step - loss: 0.8425 - accuracy: 0.6937 - val_loss: 1.0642 - val_accuracy: 0.6064\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 209s 465ms/step - loss: 0.8199 - accuracy: 0.6975 - val_loss: 1.0807 - val_accuracy: 0.6031\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 223s 497ms/step - loss: 0.7985 - accuracy: 0.7101 - val_loss: 1.0734 - val_accuracy: 0.6087\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 227s 505ms/step - loss: 0.7752 - accuracy: 0.7182 - val_loss: 1.0707 - val_accuracy: 0.6094\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 215s 479ms/step - loss: 0.7520 - accuracy: 0.7267 - val_loss: 1.0676 - val_accuracy: 0.6122\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 214s 477ms/step - loss: 0.7256 - accuracy: 0.7361 - val_loss: 1.0888 - val_accuracy: 0.6097\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 227s 506ms/step - loss: 0.7065 - accuracy: 0.7429 - val_loss: 1.0760 - val_accuracy: 0.6129\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 216s 482ms/step - loss: 0.6818 - accuracy: 0.7513 - val_loss: 1.0797 - val_accuracy: 0.6175\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 210s 469ms/step - loss: 0.6639 - accuracy: 0.7592 - val_loss: 1.0969 - val_accuracy: 0.6122\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 208s 464ms/step - loss: 0.6431 - accuracy: 0.7667 - val_loss: 1.0840 - val_accuracy: 0.6194\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 210s 470ms/step - loss: 0.6214 - accuracy: 0.7734 - val_loss: 1.1006 - val_accuracy: 0.6190\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 214s 477ms/step - loss: 0.6059 - accuracy: 0.7794 - val_loss: 1.0869 - val_accuracy: 0.6182\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 213s 475ms/step - loss: 0.5875 - accuracy: 0.7891 - val_loss: 1.1140 - val_accuracy: 0.6196\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 216s 482ms/step - loss: 0.5630 - accuracy: 0.7965 - val_loss: 1.1076 - val_accuracy: 0.6204\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 215s 479ms/step - loss: 0.5466 - accuracy: 0.8003 - val_loss: 1.1119 - val_accuracy: 0.6215\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 215s 480ms/step - loss: 0.5318 - accuracy: 0.8102 - val_loss: 1.1162 - val_accuracy: 0.6214\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 269s 600ms/step - loss: 0.5104 - accuracy: 0.8191 - val_loss: 1.1230 - val_accuracy: 0.6182\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 331s 740ms/step - loss: 0.4930 - accuracy: 0.8230 - val_loss: 1.1324 - val_accuracy: 0.6235\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 216s 482ms/step - loss: 0.4826 - accuracy: 0.8265 - val_loss: 1.1571 - val_accuracy: 0.6225\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 215s 481ms/step - loss: 0.4593 - accuracy: 0.8327 - val_loss: 1.1542 - val_accuracy: 0.6217\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 219s 488ms/step - loss: 0.4498 - accuracy: 0.8401 - val_loss: 1.1523 - val_accuracy: 0.6250\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 213s 476ms/step - loss: 0.4396 - accuracy: 0.8418 - val_loss: 1.1623 - val_accuracy: 0.6242\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 218s 486ms/step - loss: 0.4241 - accuracy: 0.8485 - val_loss: 1.1924 - val_accuracy: 0.6215\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 218s 486ms/step - loss: 0.4111 - accuracy: 0.8513 - val_loss: 1.1800 - val_accuracy: 0.6257\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 220s 491ms/step - loss: 0.4047 - accuracy: 0.8553 - val_loss: 1.1768 - val_accuracy: 0.6243\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 224s 501ms/step - loss: 0.3901 - accuracy: 0.8582 - val_loss: 1.1887 - val_accuracy: 0.6249\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = 'data/train'\n",
    "val_dir = 'data/test'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "# emotion_model.load_weights('emotion_model.h5')\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)\n",
    "emotion_model.save_weights('emotion_model.h5')\n",
    "\n",
    "# start the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2gray_frame)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87972bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
